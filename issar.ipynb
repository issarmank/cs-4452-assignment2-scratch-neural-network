{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "441dccaa",
   "metadata": {},
   "source": [
    "## Problem 1: Classification Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6dbeb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3c4112",
   "metadata": {},
   "source": [
    "### Q1 (10 points): Implement Instance Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbdc219b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstanceNorm1D:\n",
    "    def __init__(self, D, eps=1e-5, device=None, dtype=torch.float32):\n",
    "        self.D = D\n",
    "        self.eps = eps\n",
    "\n",
    "        # parameters (learned)\n",
    "        self.gamma = torch.ones(D, device=device, dtype=dtype)\n",
    "        self.beta  = torch.zeros(D, device=device, dtype=dtype)\n",
    "\n",
    "        # gradients\n",
    "        self.dgamma = torch.zeros_like(self.gamma)\n",
    "        self.dbeta  = torch.zeros_like(self.beta)\n",
    "\n",
    "        # cache\n",
    "        self._cache = None\n",
    "\n",
    "    def forward(self, z):\n",
    "        assert z.dim() == 2 and z.size(1) == self.D, \"Expected (B, D) input\"\n",
    "\n",
    "        mu = z.mean(dim=1, keepdim=True)                         # (B, 1)\n",
    "        var = ((z - mu) ** 2).mean(dim=1, keepdim=True)          # (B, 1)\n",
    "        invstd = torch.rsqrt(var + self.eps)                     # (B, 1)\n",
    "        zhat = (z - mu) * invstd                                 # (B, D)\n",
    "\n",
    "        a = zhat * self.gamma.view(1, -1) + self.beta.view(1, -1)\n",
    "\n",
    "        # cache for backward\n",
    "        self._cache = (zhat, invstd)\n",
    "        return a\n",
    "\n",
    "    def backward(self, da):\n",
    "        zhat, invstd = self._cache\n",
    "        B, D = da.shape\n",
    "        assert D == self.D, \"Mismatched feature dimension\"\n",
    "\n",
    "        # parameter gradients\n",
    "        self.dbeta = da.sum(dim=0)               # (D,)\n",
    "        self.dgamma = (da * zhat).sum(dim=0)     # (D,)\n",
    "\n",
    "        # dL/dzhat\n",
    "        dzhat = da * self.gamma.view(1, -1)      # (B, D)\n",
    "\n",
    "        sum_dzhat = dzhat.sum(dim=1, keepdim=True)                # (B, 1)\n",
    "        sum_dzhat_zhat = (dzhat * zhat).sum(dim=1, keepdim=True)  # (B, 1)\n",
    "\n",
    "        dz = (invstd / D) * (D * dzhat - sum_dzhat - zhat * sum_dzhat_zhat)  # (B, D)\n",
    "        return dz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557a023d",
   "metadata": {},
   "source": [
    "### Q2 (10 points): Implement Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fbf2aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, p=0.5):\n",
    "        assert 0.0 <= p < 1.0, \"p must be in [0, 1)\"\n",
    "        self.p = float(p)\n",
    "        self._mask = None\n",
    "        self._train = True  # default mode like PyTorch modules\n",
    "\n",
    "    def train(self):\n",
    "        self._train = True\n",
    "\n",
    "    def eval(self):\n",
    "        self._train = False\n",
    "\n",
    "    def forward(self, z):\n",
    "        if (not self._train) or self.p == 0.0:\n",
    "            self._mask = None\n",
    "            return z\n",
    "\n",
    "        keep_prob = 1.0 - self.p\n",
    "        # mask is 1 with prob keep_prob, else 0\n",
    "        self._mask = (torch.rand_like(z) < keep_prob).to(z.dtype)\n",
    "        a = (self._mask * z) / keep_prob\n",
    "        return a\n",
    "\n",
    "    def backward(self, da):\n",
    "        if (not self._train) or self.p == 0.0:\n",
    "            return da\n",
    "\n",
    "        keep_prob = 1.0 - self.p\n",
    "        dz = (self._mask / keep_prob) * da\n",
    "        return dz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0973410",
   "metadata": {},
   "source": [
    "### Q3 (10 points): Implement Softmax Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c32cf328",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxCrossEntropy:\n",
    "    def __init__(self):\n",
    "        self._cache = None  # (yhat, y)\n",
    "\n",
    "    def forward(self, z, y):\n",
    "        assert z.dim() == 2, \"z must be (B, C)\"\n",
    "        assert y.dim() == 1 and y.size(0) == z.size(0), \"y must be (B,)\"\n",
    "\n",
    "        B, C = z.shape\n",
    "\n",
    "        # stable softmax: subtract row-wise max\n",
    "        z_shift = z - z.max(dim=1, keepdim=True).values           # (B, C)\n",
    "        exp_z = torch.exp(z_shift)                                 # (B, C)\n",
    "        yhat = exp_z / exp_z.sum(dim=1, keepdim=True)              # (B, C)\n",
    "\n",
    "        # cross-entropy: -mean(log prob of correct class)\n",
    "        correct_probs = yhat[torch.arange(B, device=z.device), y]  # (B,)\n",
    "        loss = -torch.log(correct_probs + 1e-12).mean()            # scalar\n",
    "\n",
    "        self._cache = (yhat, y)\n",
    "        return loss\n",
    "\n",
    "    def backward(self):\n",
    "        yhat, y = self._cache\n",
    "        B, C = yhat.shape\n",
    "\n",
    "        dz = yhat.clone()\n",
    "        dz[torch.arange(B, device=dz.device), y] -= 1.0\n",
    "        dz /= B\n",
    "        return dz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a137fea6",
   "metadata": {},
   "source": [
    "### Q4 (10 points): Implement Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f2abf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def __init__(self):\n",
    "        self._cache = None # tanh(z)\n",
    "\n",
    "    def forward(self, z):\n",
    "        a = torch.tanh(z)\n",
    "        self._cache = a  # cache the output for backward\n",
    "        return a\n",
    "    \n",
    "    def backward(self, da):\n",
    "        assert self._cache is not None, \"Must call forward before backward\"\n",
    "        a = self._cache\n",
    "        dz = da * (1.0 - a ** 2)\n",
    "        return dz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a95f1f9",
   "metadata": {},
   "source": [
    "### Q5 (10 points): Build and Train a Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e5d761",
   "metadata": {},
   "source": [
    "Reusing helpers from assignment2.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ba71a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_spiral(n_per_class=200, n_classes=2, noise=0.2, rotations=1.0, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    N = n_per_class * n_classes\n",
    "    X = np.zeros((N, 2), dtype=np.float32)\n",
    "    y = np.zeros((N,), dtype=np.int64)\n",
    "\n",
    "    # radius grows from 0 to 1\n",
    "    r = np.linspace(0.0, 1.0, n_per_class, dtype=np.float32)\n",
    "\n",
    "    for c in range(n_classes):\n",
    "        # angle: start offset for each class + rotations * 2pi * r\n",
    "        offset = (2.0 * np.pi / n_classes) * c\n",
    "        theta = offset + rotations * 2.0 * np.pi * r\n",
    "\n",
    "        # base spiral\n",
    "        xs = r * np.cos(theta)\n",
    "        ys = r * np.sin(theta)\n",
    "\n",
    "        # add noise\n",
    "        xs += rng.normal(0.0, noise, size=n_per_class).astype(np.float32)\n",
    "        ys += rng.normal(0.0, noise, size=n_per_class).astype(np.float32)\n",
    "\n",
    "        idx0 = c * n_per_class\n",
    "        idx1 = (c + 1) * n_per_class\n",
    "        X[idx0:idx1, 0] = xs\n",
    "        X[idx0:idx1, 1] = ys\n",
    "        y[idx0:idx1] = c\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def train_val_split(X, y, val_ratio=0.2, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = np.arange(len(X))\n",
    "    rng.shuffle(idx)\n",
    "    n_val = int(len(X) * val_ratio)\n",
    "    val_idx = idx[:n_val]\n",
    "    tr_idx = idx[n_val:]\n",
    "    return X[tr_idx], y[tr_idx], X[val_idx], y[val_idx]\n",
    "\n",
    "@torch.no_grad()\n",
    "def accuracy_from_logits(logits, y):\n",
    "    pred = torch.argmax(logits, dim=1)\n",
    "    return (pred == y).float().mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a38fc4",
   "metadata": {},
   "source": [
    "#### Linear Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acdd1f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, in_dim, out_dim, device=None, dtype=torch.float32):\n",
    "        self.in_dim = int(in_dim)\n",
    "        self.out_dim = int(out_dim)\n",
    "\n",
    "        # Kaiming-ish small init (good enough for tanh; keep simple)\n",
    "        w_scale = (1.0 / np.sqrt(self.in_dim)).astype(np.float32) if isinstance(self.in_dim, np.ndarray) else (1.0 / np.sqrt(self.in_dim))\n",
    "        self.W = torch.randn(self.in_dim, self.out_dim, device=device, dtype=dtype) * w_scale\n",
    "        self.b = torch.zeros(self.out_dim, device=device, dtype=dtype)\n",
    "\n",
    "        self.dW = torch.zeros_like(self.W)\n",
    "        self.db = torch.zeros_like(self.b)\n",
    "\n",
    "        self._cache_x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.dim() == 2 and x.size(1) == self.in_dim, \"Expected (B, in_dim)\"\n",
    "        self._cache_x = x\n",
    "        return x @ self.W + self.b.view(1, -1)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        x = self._cache_x\n",
    "        assert x is not None, \"Must call forward() before backward()\"\n",
    "\n",
    "        # grads\n",
    "        self.dW = x.t() @ dout                      # (in_dim, out_dim)\n",
    "        self.db = dout.sum(dim=0)                   # (out_dim,)\n",
    "        dx = dout @ self.W.t()                      # (B, in_dim)\n",
    "        return dx\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.dW.zero_()\n",
    "        self.db.zero_()\n",
    "\n",
    "    def step(self, lr):\n",
    "        self.W -= lr * self.dW\n",
    "        self.b -= lr * self.db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd38f25a",
   "metadata": {},
   "source": [
    "#### MLP Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edaf1428",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \"\"\"\n",
    "    2-hidden-layer MLP:\n",
    "      Linear -> InstanceNorm -> Tanh -> Dropout\n",
    "      Linear -> InstanceNorm -> Tanh -> Dropout\n",
    "      Linear -> logits\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim=2, hidden_dim=64, out_dim=2, p_drop=0.2, device=None, dtype=torch.float32):\n",
    "        self.l1 = Linear(in_dim, hidden_dim, device=device, dtype=dtype)\n",
    "        self.n1 = InstanceNorm1D(hidden_dim, device=device, dtype=dtype)\n",
    "        self.a1 = Tanh()\n",
    "        self.d1 = Dropout(p_drop)\n",
    "\n",
    "        self.l2 = Linear(hidden_dim, hidden_dim, device=device, dtype=dtype)\n",
    "        self.n2 = InstanceNorm1D(hidden_dim, device=device, dtype=dtype)\n",
    "        self.a2 = Tanh()\n",
    "        self.d2 = Dropout(p_drop)\n",
    "\n",
    "        self.l3 = Linear(hidden_dim, out_dim, device=device, dtype=dtype)\n",
    "\n",
    "        self._train = True\n",
    "\n",
    "    def train(self):\n",
    "        self._train = True\n",
    "        self.d1.train()\n",
    "        self.d2.train()\n",
    "\n",
    "    def eval(self):\n",
    "        self._train = False\n",
    "        self.d1.eval()\n",
    "        self.d2.eval()\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.l1.forward(x)\n",
    "        z = self.n1.forward(z)\n",
    "        z = self.a1.forward(z)\n",
    "        z = self.d1.forward(z)\n",
    "\n",
    "        z = self.l2.forward(z)\n",
    "        z = self.n2.forward(z)\n",
    "        z = self.a2.forward(z)\n",
    "        z = self.d2.forward(z)\n",
    "\n",
    "        logits = self.l3.forward(z)\n",
    "        return logits\n",
    "\n",
    "    def backward(self, dlogits):\n",
    "        dz = self.l3.backward(dlogits)\n",
    "\n",
    "        dz = self.d2.backward(dz)\n",
    "        dz = self.a2.backward(dz)\n",
    "        dz = self.n2.backward(dz)\n",
    "        dz = self.l2.backward(dz)\n",
    "\n",
    "        dz = self.d1.backward(dz)\n",
    "        dz = self.a1.backward(dz)\n",
    "        dz = self.n1.backward(dz)\n",
    "        dz = self.l1.backward(dz)\n",
    "        return dz\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.l1.zero_grad()\n",
    "        self.l2.zero_grad()\n",
    "        self.l3.zero_grad()\n",
    "        # InstanceNorm params are plain tensors; nothing to \"zero\" besides overwriting dgamma/dbeta in backward\n",
    "        # (they are assigned each backward call)\n",
    "\n",
    "    def step(self, lr):\n",
    "        # linear params\n",
    "        self.l1.step(lr)\n",
    "        self.l2.step(lr)\n",
    "        self.l3.step(lr)\n",
    "\n",
    "        # instance norm params\n",
    "        self.n1.gamma -= lr * self.n1.dgamma\n",
    "        self.n1.beta  -= lr * self.n1.dbeta\n",
    "        self.n2.gamma -= lr * self.n2.dgamma\n",
    "        self.n2.beta  -= lr * self.n2.dbeta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128c472c",
   "metadata": {},
   "source": [
    "Generating required data, model, & loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b8e4387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "X_np, y_np = generate_spiral(n_per_class=150, n_classes=2, noise=0.05, rotations=1.5, seed=42)\n",
    "Xtr, ytr, Xva, yva = train_val_split(X_np, y_np, val_ratio=0.2, seed=0)\n",
    "\n",
    "Xtr_t = torch.tensor(Xtr, dtype=torch.float32)\n",
    "ytr_t = torch.tensor(ytr, dtype=torch.int64)\n",
    "Xva_t = torch.tensor(Xva, dtype=torch.float32)\n",
    "yva_t = torch.tensor(yva, dtype=torch.int64)\n",
    "\n",
    "# model + loss\n",
    "model = MLP(in_dim=2, hidden_dim=64, out_dim=2, p_drop=0.2)\n",
    "loss_fn = SoftmaxCrossEntropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207ef2be",
   "metadata": {},
   "source": [
    "Training loop (with required hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bfeeef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1 | train loss 0.7353 | train acc 0.559 | val acc 0.550\n",
      "Epoch  200 | train loss 0.6381 | train acc 0.635 | val acc 0.617\n",
      "Epoch  400 | train loss 0.5926 | train acc 0.648 | val acc 0.683\n",
      "Epoch  600 | train loss 0.4985 | train acc 0.725 | val acc 0.733\n",
      "Epoch  800 | train loss 0.2723 | train acc 0.854 | val acc 0.867\n",
      "Epoch 1000 | train loss 0.2524 | train acc 0.889 | val acc 0.917\n",
      "Epoch 1200 | train loss 0.3024 | train acc 0.875 | val acc 0.917\n",
      "Epoch 1400 | train loss 0.2233 | train acc 0.906 | val acc 0.883\n",
      "Epoch 1600 | train loss 0.2256 | train acc 0.928 | val acc 0.917\n",
      "Epoch 1800 | train loss 0.1648 | train acc 0.947 | val acc 0.917\n",
      "Epoch 2000 | train loss 0.1493 | train acc 0.941 | val acc 0.917\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "epochs = 2000\n",
    "batch_size = 64\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "\n",
    "    # shuffle\n",
    "    idx = rng.permutation(len(Xtr_t))\n",
    "    Xtr_t = Xtr_t[idx]\n",
    "    ytr_t = ytr_t[idx]\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for start in range(0, len(Xtr_t), batch_size):\n",
    "        xb = Xtr_t[start:start + batch_size]\n",
    "        yb = ytr_t[start:start + batch_size]\n",
    "\n",
    "        # zero grads\n",
    "        model.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        logits = model.forward(xb)\n",
    "\n",
    "        # loss\n",
    "        loss = loss_fn.forward(logits, yb)\n",
    "\n",
    "        # backward\n",
    "        dlogits = loss_fn.backward()\n",
    "        model.backward(dlogits)\n",
    "\n",
    "        # update (SGD, no weight decay)\n",
    "        model.step(lr)\n",
    "\n",
    "        total_loss += float(loss.detach().cpu())\n",
    "        total_acc += accuracy_from_logits(logits, yb)\n",
    "        n_batches += 1\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_logits = model.forward(Xva_t)\n",
    "        val_acc = accuracy_from_logits(val_logits, yva_t)\n",
    "\n",
    "    if epoch % 200 == 0 or epoch == 1:\n",
    "        print(\n",
    "            f\"Epoch {epoch:4d} | \"\n",
    "            f\"train loss {total_loss / n_batches:.4f} | \"\n",
    "            f\"train acc {total_acc / n_batches:.3f} | \"\n",
    "            f\"val acc {val_acc:.3f}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbace0e2",
   "metadata": {},
   "source": [
    "Final report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abb753b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training accuracy:   0.967\n",
      "Final validation accuracy: 0.917\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    final_train_acc = accuracy_from_logits(model.forward(Xtr_t), ytr_t)\n",
    "    final_val_acc = accuracy_from_logits(model.forward(Xva_t), yva_t)\n",
    "\n",
    "print(f\"Final training accuracy:   {final_train_acc:.3f}\")\n",
    "print(f\"Final validation accuracy: {final_val_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1f29f2",
   "metadata": {},
   "source": [
    "#### Q6 (25 points): InstanceNorm vs BatchNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20610933",
   "metadata": {},
   "source": [
    "Replacing InstanceNorm with BatchNormID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8d31b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BatchNorm1D (feature dimension D on a (B, D) tensor)\n",
    "class BatchNorm1D:\n",
    "    def __init__(self, D, eps=1e-5, momentum=0.9, device=None, dtype=torch.float32):\n",
    "        self.D = int(D)\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "\n",
    "        # learned affine params\n",
    "        self.gamma = torch.ones(self.D, device=device, dtype=dtype)\n",
    "        self.beta  = torch.zeros(self.D, device=device, dtype=dtype)\n",
    "\n",
    "        # gradients\n",
    "        self.dgamma = torch.zeros_like(self.gamma)\n",
    "        self.dbeta  = torch.zeros_like(self.beta)\n",
    "\n",
    "        # running stats (for eval)\n",
    "        self.running_mean = torch.zeros(self.D, device=device, dtype=dtype)\n",
    "        self.running_var  = torch.ones(self.D, device=device, dtype=dtype)\n",
    "\n",
    "        self._train = True\n",
    "        self._cache = None  # (xhat, invstd)\n",
    "\n",
    "    def train(self):\n",
    "        self._train = True\n",
    "\n",
    "    def eval(self):\n",
    "        self._train = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.dim() == 2 and x.size(1) == self.D, \"Expected (B, D)\"\n",
    "        if self._train:\n",
    "            mu = x.mean(dim=0, keepdim=True)                          # (1, D)\n",
    "            var = ((x - mu) ** 2).mean(dim=0, keepdim=True)           # (1, D)\n",
    "            invstd = torch.rsqrt(var + self.eps)                      # (1, D)\n",
    "            xhat = (x - mu) * invstd                                  # (B, D)\n",
    "\n",
    "            # update running stats (detach so they don't build graphs)\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mu.squeeze(0).detach()\n",
    "            self.running_var  = self.momentum * self.running_var  + (1 - self.momentum) * var.squeeze(0).detach()\n",
    "\n",
    "            self._cache = (xhat, invstd)\n",
    "        else:\n",
    "            mu = self.running_mean.view(1, -1)                         # (1, D)\n",
    "            var = self.running_var.view(1, -1)                         # (1, D)\n",
    "            invstd = torch.rsqrt(var + self.eps)                       # (1, D)\n",
    "            xhat = (x - mu) * invstd                                   # (B, D)\n",
    "            self._cache = None  # no backward in eval typically\n",
    "\n",
    "        out = xhat * self.gamma.view(1, -1) + self.beta.view(1, -1)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        assert self._cache is not None, \"BatchNorm backward called without forward() in train mode\"\n",
    "        xhat, invstd = self._cache\n",
    "        B, D = dout.shape\n",
    "        assert D == self.D\n",
    "\n",
    "        # affine grads\n",
    "        self.dbeta = dout.sum(dim=0)\n",
    "        self.dgamma = (dout * xhat).sum(dim=0)\n",
    "\n",
    "        # backprop through BN (alternate simplified formula)\n",
    "        dxhat = dout * self.gamma.view(1, -1)                          # (B, D)\n",
    "        sum_dxhat = dxhat.sum(dim=0, keepdim=True)                     # (1, D)\n",
    "        sum_dxhat_xhat = (dxhat * xhat).sum(dim=0, keepdim=True)       # (1, D)\n",
    "\n",
    "        dx = (invstd / B) * (B * dxhat - sum_dxhat - xhat * sum_dxhat_xhat)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66adcfa5",
   "metadata": {},
   "source": [
    "BatchNorm based MLP variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6deaac35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_BN:\n",
    "    \"\"\"\n",
    "    Same architecture as MLP, but with BatchNorm instead of InstanceNorm.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim=2, hidden_dim=64, out_dim=2, p_drop=0.2, device=None, dtype=torch.float32):\n",
    "        self.l1 = Linear(in_dim, hidden_dim, device=device, dtype=dtype)\n",
    "        self.n1 = BatchNorm1D(hidden_dim, device=device, dtype=dtype)\n",
    "        self.a1 = Tanh()\n",
    "        self.d1 = Dropout(p_drop)\n",
    "\n",
    "        self.l2 = Linear(hidden_dim, hidden_dim, device=device, dtype=dtype)\n",
    "        self.n2 = BatchNorm1D(hidden_dim, device=device, dtype=dtype)\n",
    "        self.a2 = Tanh()\n",
    "        self.d2 = Dropout(p_drop)\n",
    "\n",
    "        self.l3 = Linear(hidden_dim, out_dim, device=device, dtype=dtype)\n",
    "\n",
    "    def train(self):\n",
    "        self.d1.train()\n",
    "        self.d2.train()\n",
    "        self.n1.train()\n",
    "        self.n2.train()\n",
    "\n",
    "    def eval(self):\n",
    "        self.d1.eval()\n",
    "        self.d2.eval()\n",
    "        self.n1.eval()\n",
    "        self.n2.eval()\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.l1.forward(x)\n",
    "        z = self.n1.forward(z)\n",
    "        z = self.a1.forward(z)\n",
    "        z = self.d1.forward(z)\n",
    "\n",
    "        z = self.l2.forward(z)\n",
    "        z = self.n2.forward(z)\n",
    "        z = self.a2.forward(z)\n",
    "        z = self.d2.forward(z)\n",
    "\n",
    "        logits = self.l3.forward(z)\n",
    "        return logits\n",
    "\n",
    "    def backward(self, dlogits):\n",
    "        dz = self.l3.backward(dlogits)\n",
    "\n",
    "        dz = self.d2.backward(dz)\n",
    "        dz = self.a2.backward(dz)\n",
    "        dz = self.n2.backward(dz)\n",
    "        dz = self.l2.backward(dz)\n",
    "\n",
    "        dz = self.d1.backward(dz)\n",
    "        dz = self.a1.backward(dz)\n",
    "        dz = self.n1.backward(dz)\n",
    "        dz = self.l1.backward(dz)\n",
    "        return dz\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.l1.zero_grad()\n",
    "        self.l2.zero_grad()\n",
    "        self.l3.zero_grad()\n",
    "\n",
    "    def step(self, lr):\n",
    "        # linear params\n",
    "        self.l1.step(lr)\n",
    "        self.l2.step(lr)\n",
    "        self.l3.step(lr)\n",
    "\n",
    "        # norm params\n",
    "        self.n1.gamma -= lr * self.n1.dgamma\n",
    "        self.n1.beta  -= lr * self.n1.dbeta\n",
    "        self.n2.gamma -= lr * self.n2.dgamma\n",
    "        self.n2.beta  -= lr * self.n2.dbeta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c892e70c",
   "metadata": {},
   "source": [
    "Training both models with same parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ec44dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loss_fn, Xtr_t, ytr_t, Xva_t, yva_t, lr=0.1, epochs=2000, batch_size=64, seed=0, print_every=200):\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "\n",
    "        idx = rng.permutation(len(Xtr_t))\n",
    "        Xsh = Xtr_t[idx]\n",
    "        ysh = ytr_t[idx]\n",
    "\n",
    "        total_loss = 0.0\n",
    "        total_acc = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for start in range(0, len(Xsh), batch_size):\n",
    "            xb = Xsh[start:start + batch_size]\n",
    "            yb = ysh[start:start + batch_size]\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            logits = model.forward(xb)\n",
    "            loss = loss_fn.forward(logits, yb)\n",
    "\n",
    "            dlogits = loss_fn.backward()\n",
    "            model.backward(dlogits)\n",
    "\n",
    "            model.step(lr)\n",
    "\n",
    "            total_loss += float(loss.detach().cpu())\n",
    "            total_acc += accuracy_from_logits(logits, yb)\n",
    "            n_batches += 1\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_logits = model.forward(Xva_t)\n",
    "            val_acc = accuracy_from_logits(val_logits, yva_t)\n",
    "\n",
    "        if epoch % print_every == 0 or epoch == 1:\n",
    "            print(\n",
    "                f\"Epoch {epoch:4d} | \"\n",
    "                f\"train loss {total_loss / n_batches:.4f} | \"\n",
    "                f\"train acc {total_acc / n_batches:.3f} | \"\n",
    "                f\"val acc {val_acc:.3f}\"\n",
    "            )\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        final_train_acc = accuracy_from_logits(model.forward(Xtr_t), ytr_t)\n",
    "        final_val_acc = accuracy_from_logits(model.forward(Xva_t), yva_t)\n",
    "\n",
    "    return final_train_acc, final_val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652d2566",
   "metadata": {},
   "source": [
    "Checking InstanceNorm vs BatchNorm comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "637e6d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training InstanceNorm model\n",
      "Epoch    1 | train loss 0.7751 | train acc 0.546 | val acc 0.533\n",
      "Epoch  200 | train loss 0.6486 | train acc 0.622 | val acc 0.600\n",
      "Epoch  400 | train loss 0.5994 | train acc 0.668 | val acc 0.667\n",
      "Epoch  600 | train loss 0.5382 | train acc 0.732 | val acc 0.650\n",
      "Epoch  800 | train loss 0.4288 | train acc 0.788 | val acc 0.900\n",
      "Epoch 1000 | train loss 0.2935 | train acc 0.902 | val acc 0.917\n",
      "Epoch 1200 | train loss 0.2620 | train acc 0.887 | val acc 0.900\n",
      "Epoch 1400 | train loss 0.1932 | train acc 0.928 | val acc 0.900\n",
      "Epoch 1600 | train loss 0.1974 | train acc 0.928 | val acc 0.900\n",
      "Epoch 1800 | train loss 0.1718 | train acc 0.935 | val acc 0.900\n",
      "Epoch 2000 | train loss 0.1642 | train acc 0.928 | val acc 0.900\n",
      "\n",
      "Training BatchNorm model\n",
      "Epoch    1 | train loss 0.7812 | train acc 0.561 | val acc 0.667\n",
      "Epoch  200 | train loss 0.6841 | train acc 0.592 | val acc 0.567\n",
      "Epoch  400 | train loss 0.6648 | train acc 0.576 | val acc 0.667\n",
      "Epoch  600 | train loss 0.6905 | train acc 0.531 | val acc 0.583\n",
      "Epoch  800 | train loss 0.6861 | train acc 0.596 | val acc 0.617\n",
      "Epoch 1000 | train loss 0.6144 | train acc 0.690 | val acc 0.617\n",
      "Epoch 1200 | train loss 0.6661 | train acc 0.596 | val acc 0.650\n",
      "Epoch 1400 | train loss 0.5437 | train acc 0.720 | val acc 0.633\n",
      "Epoch 1600 | train loss 0.5829 | train acc 0.673 | val acc 0.750\n",
      "Epoch 1800 | train loss 0.5273 | train acc 0.712 | val acc 0.750\n",
      "Epoch 2000 | train loss 0.4036 | train acc 0.820 | val acc 0.883\n",
      "\n",
      "Final report\n",
      "InstanceNorm  final train acc: 0.967 | final val acc: 0.900\n",
      "BatchNorm     final train acc: 0.954 | final val acc: 0.883\n"
     ]
    }
   ],
   "source": [
    "# Ensure fair comparison: same initial seeds for weights\n",
    "torch.manual_seed(0)\n",
    "model_in = MLP(in_dim=2, hidden_dim=64, out_dim=2, p_drop=0.2)\n",
    "loss_in = SoftmaxCrossEntropy()\n",
    "\n",
    "torch.manual_seed(0)\n",
    "model_bn = MLP_BN(in_dim=2, hidden_dim=64, out_dim=2, p_drop=0.2)\n",
    "loss_bn = SoftmaxCrossEntropy()\n",
    "\n",
    "print(\"Training InstanceNorm model\")\n",
    "in_train_acc, in_val_acc = train_model(model_in, loss_in, Xtr_t, ytr_t, Xva_t, yva_t,\n",
    "                                       lr=0.1, epochs=2000, batch_size=64, seed=0)\n",
    "\n",
    "print(\"\\nTraining BatchNorm model\")\n",
    "bn_train_acc, bn_val_acc = train_model(model_bn, loss_bn, Xtr_t, ytr_t, Xva_t, yva_t,\n",
    "                                       lr=0.1, epochs=2000, batch_size=64, seed=0)\n",
    "\n",
    "print(\"\\nFinal report\")\n",
    "print(f\"InstanceNorm  final train acc: {in_train_acc:.3f} | final val acc: {in_val_acc:.3f}\")\n",
    "print(f\"BatchNorm     final train acc: {bn_train_acc:.3f} | final val acc: {bn_val_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dff1302",
   "metadata": {},
   "source": [
    "#### What I observed\n",
    "\n",
    "InstanceNorm training and validation is slightly higher then BatchNorm and this can be because BatchNorm is slightly nore nosier with small batch sizes and InstanceNorm tends to be more stable when stats are unreliable because it normalizes per-sample. \n",
    "\n",
    "InstanceNorm computes mean/variance for each sample while BatchNorm computes across the batch for each feature which can have different affects according to batch sizes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82805c5c",
   "metadata": {},
   "source": [
    "#### Q7 (25 points): InstanceNorm With and Without Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8334abad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (DS3000env)",
   "language": "python",
   "name": "ds3000env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
