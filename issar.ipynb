{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "441dccaa",
   "metadata": {},
   "source": [
    "## Problem 1: Classification Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6dbeb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3c4112",
   "metadata": {},
   "source": [
    "### Q1 (10 points): Implement Instance Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbdc219b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstanceNorm1D:\n",
    "    def __init__(self, D, eps=1e-5, device=None, dtype=torch.float32):\n",
    "        self.D = D\n",
    "        self.eps = eps\n",
    "\n",
    "        # parameters (learned)\n",
    "        self.gamma = torch.ones(D, device=device, dtype=dtype)\n",
    "        self.beta  = torch.zeros(D, device=device, dtype=dtype)\n",
    "\n",
    "        # gradients\n",
    "        self.dgamma = torch.zeros_like(self.gamma)\n",
    "        self.dbeta  = torch.zeros_like(self.beta)\n",
    "\n",
    "        # cache\n",
    "        self._cache = None\n",
    "\n",
    "    def forward(self, z):\n",
    "        assert z.dim() == 2 and z.size(1) == self.D, \"Expected (B, D) input\"\n",
    "\n",
    "        mu = z.mean(dim=1, keepdim=True)                         # (B, 1)\n",
    "        var = ((z - mu) ** 2).mean(dim=1, keepdim=True)          # (B, 1)\n",
    "        invstd = torch.rsqrt(var + self.eps)                     # (B, 1)\n",
    "        zhat = (z - mu) * invstd                                 # (B, D)\n",
    "\n",
    "        a = zhat * self.gamma.view(1, -1) + self.beta.view(1, -1)\n",
    "\n",
    "        # cache for backward\n",
    "        self._cache = (zhat, invstd)\n",
    "        return a\n",
    "\n",
    "    def backward(self, da):\n",
    "        zhat, invstd = self._cache\n",
    "        B, D = da.shape\n",
    "        assert D == self.D, \"Mismatched feature dimension\"\n",
    "\n",
    "        # parameter gradients\n",
    "        self.dbeta = da.sum(dim=0)               # (D,)\n",
    "        self.dgamma = (da * zhat).sum(dim=0)     # (D,)\n",
    "\n",
    "        # dL/dzhat\n",
    "        dzhat = da * self.gamma.view(1, -1)      # (B, D)\n",
    "\n",
    "        sum_dzhat = dzhat.sum(dim=1, keepdim=True)                # (B, 1)\n",
    "        sum_dzhat_zhat = (dzhat * zhat).sum(dim=1, keepdim=True)  # (B, 1)\n",
    "\n",
    "        dz = (invstd / D) * (D * dzhat - sum_dzhat - zhat * sum_dzhat_zhat)  # (B, D)\n",
    "        return dz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557a023d",
   "metadata": {},
   "source": [
    "### Q2 (10 points): Implement Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fbf2aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, p=0.5):\n",
    "        assert 0.0 <= p < 1.0, \"p must be in [0, 1)\"\n",
    "        self.p = float(p)\n",
    "        self._mask = None\n",
    "        self._train = True  # default mode like PyTorch modules\n",
    "\n",
    "    def train(self):\n",
    "        self._train = True\n",
    "\n",
    "    def eval(self):\n",
    "        self._train = False\n",
    "\n",
    "    def forward(self, z):\n",
    "        if (not self._train) or self.p == 0.0:\n",
    "            self._mask = None\n",
    "            return z\n",
    "\n",
    "        keep_prob = 1.0 - self.p\n",
    "        # mask is 1 with prob keep_prob, else 0\n",
    "        self._mask = (torch.rand_like(z) < keep_prob).to(z.dtype)\n",
    "        a = (self._mask * z) / keep_prob\n",
    "        return a\n",
    "\n",
    "    def backward(self, da):\n",
    "        if (not self._train) or self.p == 0.0:\n",
    "            return da\n",
    "\n",
    "        keep_prob = 1.0 - self.p\n",
    "        dz = (self._mask / keep_prob) * da\n",
    "        return dz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0973410",
   "metadata": {},
   "source": [
    "### Q3 (10 points): Implement Softmax Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c32cf328",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxCrossEntropy:\n",
    "    def __init__(self):\n",
    "        self._cache = None  # (yhat, y)\n",
    "\n",
    "    def forward(self, z, y):\n",
    "        assert z.dim() == 2, \"z must be (B, C)\"\n",
    "        assert y.dim() == 1 and y.size(0) == z.size(0), \"y must be (B,)\"\n",
    "\n",
    "        B, C = z.shape\n",
    "\n",
    "        # stable softmax: subtract row-wise max\n",
    "        z_shift = z - z.max(dim=1, keepdim=True).values           # (B, C)\n",
    "        exp_z = torch.exp(z_shift)                                 # (B, C)\n",
    "        yhat = exp_z / exp_z.sum(dim=1, keepdim=True)              # (B, C)\n",
    "\n",
    "        # cross-entropy: -mean(log prob of correct class)\n",
    "        correct_probs = yhat[torch.arange(B, device=z.device), y]  # (B,)\n",
    "        loss = -torch.log(correct_probs + 1e-12).mean()            # scalar\n",
    "\n",
    "        self._cache = (yhat, y)\n",
    "        return loss\n",
    "\n",
    "    def backward(self):\n",
    "        yhat, y = self._cache\n",
    "        B, C = yhat.shape\n",
    "\n",
    "        dz = yhat.clone()\n",
    "        dz[torch.arange(B, device=dz.device), y] -= 1.0\n",
    "        dz /= B\n",
    "        return dz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a137fea6",
   "metadata": {},
   "source": [
    "### Q4 (10 points): Implement Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f2abf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def __init__(self):\n",
    "        self._cache = None # tanh(z)\n",
    "\n",
    "    def forward(self, z):\n",
    "        a = torch.tanh(z)\n",
    "        self._cache = a  # cache the output for backward\n",
    "        return a\n",
    "    \n",
    "    def backward(self, da):\n",
    "        assert self._cache is not None, \"Must call forward before backward\"\n",
    "        a = self._cache\n",
    "        dz = da * (1.0 - a ** 2)\n",
    "        return dz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a95f1f9",
   "metadata": {},
   "source": [
    "### Q5 (10 points): Build and Train a Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e5d761",
   "metadata": {},
   "source": [
    "Reusing helpers from assignment2.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ba71a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_spiral(n_per_class=200, n_classes=2, noise=0.2, rotations=1.0, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    N = n_per_class * n_classes\n",
    "    X = np.zeros((N, 2), dtype=np.float32)\n",
    "    y = np.zeros((N,), dtype=np.int64)\n",
    "\n",
    "    # radius grows from 0 to 1\n",
    "    r = np.linspace(0.0, 1.0, n_per_class, dtype=np.float32)\n",
    "\n",
    "    for c in range(n_classes):\n",
    "        # angle: start offset for each class + rotations * 2pi * r\n",
    "        offset = (2.0 * np.pi / n_classes) * c\n",
    "        theta = offset + rotations * 2.0 * np.pi * r\n",
    "\n",
    "        # base spiral\n",
    "        xs = r * np.cos(theta)\n",
    "        ys = r * np.sin(theta)\n",
    "\n",
    "        # add noise\n",
    "        xs += rng.normal(0.0, noise, size=n_per_class).astype(np.float32)\n",
    "        ys += rng.normal(0.0, noise, size=n_per_class).astype(np.float32)\n",
    "\n",
    "        idx0 = c * n_per_class\n",
    "        idx1 = (c + 1) * n_per_class\n",
    "        X[idx0:idx1, 0] = xs\n",
    "        X[idx0:idx1, 1] = ys\n",
    "        y[idx0:idx1] = c\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def train_val_split(X, y, val_ratio=0.2, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = np.arange(len(X))\n",
    "    rng.shuffle(idx)\n",
    "    n_val = int(len(X) * val_ratio)\n",
    "    val_idx = idx[:n_val]\n",
    "    tr_idx = idx[n_val:]\n",
    "    return X[tr_idx], y[tr_idx], X[val_idx], y[val_idx]\n",
    "\n",
    "@torch.no_grad()\n",
    "def accuracy_from_logits(logits, y):\n",
    "    pred = torch.argmax(logits, dim=1)\n",
    "    return (pred == y).float().mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a38fc4",
   "metadata": {},
   "source": [
    "#### Linear Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acdd1f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, in_dim, out_dim, device=None, dtype=torch.float32):\n",
    "        self.in_dim = int(in_dim)\n",
    "        self.out_dim = int(out_dim)\n",
    "\n",
    "        # Kaiming-ish small init (good enough for tanh; keep simple)\n",
    "        w_scale = (1.0 / np.sqrt(self.in_dim)).astype(np.float32) if isinstance(self.in_dim, np.ndarray) else (1.0 / np.sqrt(self.in_dim))\n",
    "        self.W = torch.randn(self.in_dim, self.out_dim, device=device, dtype=dtype) * w_scale\n",
    "        self.b = torch.zeros(self.out_dim, device=device, dtype=dtype)\n",
    "\n",
    "        self.dW = torch.zeros_like(self.W)\n",
    "        self.db = torch.zeros_like(self.b)\n",
    "\n",
    "        self._cache_x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.dim() == 2 and x.size(1) == self.in_dim, \"Expected (B, in_dim)\"\n",
    "        self._cache_x = x\n",
    "        return x @ self.W + self.b.view(1, -1)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        x = self._cache_x\n",
    "        assert x is not None, \"Must call forward() before backward()\"\n",
    "\n",
    "        # grads\n",
    "        self.dW = x.t() @ dout                      # (in_dim, out_dim)\n",
    "        self.db = dout.sum(dim=0)                   # (out_dim,)\n",
    "        dx = dout @ self.W.t()                      # (B, in_dim)\n",
    "        return dx\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.dW.zero_()\n",
    "        self.db.zero_()\n",
    "\n",
    "    def step(self, lr):\n",
    "        self.W -= lr * self.dW\n",
    "        self.b -= lr * self.db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd38f25a",
   "metadata": {},
   "source": [
    "#### MLP Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edaf1428",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \"\"\"\n",
    "    2-hidden-layer MLP:\n",
    "      Linear -> InstanceNorm -> Tanh -> Dropout\n",
    "      Linear -> InstanceNorm -> Tanh -> Dropout\n",
    "      Linear -> logits\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim=2, hidden_dim=64, out_dim=2, p_drop=0.2, device=None, dtype=torch.float32):\n",
    "        self.l1 = Linear(in_dim, hidden_dim, device=device, dtype=dtype)\n",
    "        self.n1 = InstanceNorm1D(hidden_dim, device=device, dtype=dtype)\n",
    "        self.a1 = Tanh()\n",
    "        self.d1 = Dropout(p_drop)\n",
    "\n",
    "        self.l2 = Linear(hidden_dim, hidden_dim, device=device, dtype=dtype)\n",
    "        self.n2 = InstanceNorm1D(hidden_dim, device=device, dtype=dtype)\n",
    "        self.a2 = Tanh()\n",
    "        self.d2 = Dropout(p_drop)\n",
    "\n",
    "        self.l3 = Linear(hidden_dim, out_dim, device=device, dtype=dtype)\n",
    "\n",
    "        self._train = True\n",
    "\n",
    "    def train(self):\n",
    "        self._train = True\n",
    "        self.d1.train()\n",
    "        self.d2.train()\n",
    "\n",
    "    def eval(self):\n",
    "        self._train = False\n",
    "        self.d1.eval()\n",
    "        self.d2.eval()\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.l1.forward(x)\n",
    "        z = self.n1.forward(z)\n",
    "        z = self.a1.forward(z)\n",
    "        z = self.d1.forward(z)\n",
    "\n",
    "        z = self.l2.forward(z)\n",
    "        z = self.n2.forward(z)\n",
    "        z = self.a2.forward(z)\n",
    "        z = self.d2.forward(z)\n",
    "\n",
    "        logits = self.l3.forward(z)\n",
    "        return logits\n",
    "\n",
    "    def backward(self, dlogits):\n",
    "        dz = self.l3.backward(dlogits)\n",
    "\n",
    "        dz = self.d2.backward(dz)\n",
    "        dz = self.a2.backward(dz)\n",
    "        dz = self.n2.backward(dz)\n",
    "        dz = self.l2.backward(dz)\n",
    "\n",
    "        dz = self.d1.backward(dz)\n",
    "        dz = self.a1.backward(dz)\n",
    "        dz = self.n1.backward(dz)\n",
    "        dz = self.l1.backward(dz)\n",
    "        return dz\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.l1.zero_grad()\n",
    "        self.l2.zero_grad()\n",
    "        self.l3.zero_grad()\n",
    "        # InstanceNorm params are plain tensors; nothing to \"zero\" besides overwriting dgamma/dbeta in backward\n",
    "        # (they are assigned each backward call)\n",
    "\n",
    "    def step(self, lr):\n",
    "        # linear params\n",
    "        self.l1.step(lr)\n",
    "        self.l2.step(lr)\n",
    "        self.l3.step(lr)\n",
    "\n",
    "        # instance norm params\n",
    "        self.n1.gamma -= lr * self.n1.dgamma\n",
    "        self.n1.beta  -= lr * self.n1.dbeta\n",
    "        self.n2.gamma -= lr * self.n2.dgamma\n",
    "        self.n2.beta  -= lr * self.n2.dbeta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128c472c",
   "metadata": {},
   "source": [
    "Generating required data, model, & loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b8e4387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "X_np, y_np = generate_spiral(n_per_class=150, n_classes=2, noise=0.05, rotations=1.5, seed=42)\n",
    "Xtr, ytr, Xva, yva = train_val_split(X_np, y_np, val_ratio=0.2, seed=0)\n",
    "\n",
    "Xtr_t = torch.tensor(Xtr, dtype=torch.float32)\n",
    "ytr_t = torch.tensor(ytr, dtype=torch.int64)\n",
    "Xva_t = torch.tensor(Xva, dtype=torch.float32)\n",
    "yva_t = torch.tensor(yva, dtype=torch.int64)\n",
    "\n",
    "# model + loss\n",
    "model = MLP(in_dim=2, hidden_dim=64, out_dim=2, p_drop=0.2)\n",
    "loss_fn = SoftmaxCrossEntropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207ef2be",
   "metadata": {},
   "source": [
    "Training loop (with required hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bfeeef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1 | train loss 0.7279 | train acc 0.574 | val acc 0.617\n",
      "Epoch  200 | train loss 0.6043 | train acc 0.702 | val acc 0.617\n",
      "Epoch  400 | train loss 0.4870 | train acc 0.755 | val acc 0.683\n",
      "Epoch  600 | train loss 0.4679 | train acc 0.742 | val acc 0.700\n",
      "Epoch  800 | train loss 0.4062 | train acc 0.781 | val acc 0.833\n",
      "Epoch 1000 | train loss 0.3385 | train acc 0.863 | val acc 0.900\n",
      "Epoch 1200 | train loss 0.2446 | train acc 0.913 | val acc 0.900\n",
      "Epoch 1400 | train loss 0.2137 | train acc 0.928 | val acc 0.900\n",
      "Epoch 1600 | train loss 0.1577 | train acc 0.948 | val acc 0.917\n",
      "Epoch 1800 | train loss 0.1558 | train acc 0.939 | val acc 0.900\n",
      "Epoch 2000 | train loss 0.1681 | train acc 0.944 | val acc 0.917\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "epochs = 2000\n",
    "batch_size = 64\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "\n",
    "    # shuffle\n",
    "    idx = rng.permutation(len(Xtr_t))\n",
    "    Xtr_t = Xtr_t[idx]\n",
    "    ytr_t = ytr_t[idx]\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for start in range(0, len(Xtr_t), batch_size):\n",
    "        xb = Xtr_t[start:start + batch_size]\n",
    "        yb = ytr_t[start:start + batch_size]\n",
    "\n",
    "        # zero grads\n",
    "        model.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        logits = model.forward(xb)\n",
    "\n",
    "        # loss\n",
    "        loss = loss_fn.forward(logits, yb)\n",
    "\n",
    "        # backward\n",
    "        dlogits = loss_fn.backward()\n",
    "        model.backward(dlogits)\n",
    "\n",
    "        # update (SGD, no weight decay)\n",
    "        model.step(lr)\n",
    "\n",
    "        total_loss += float(loss.detach().cpu())\n",
    "        total_acc += accuracy_from_logits(logits, yb)\n",
    "        n_batches += 1\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_logits = model.forward(Xva_t)\n",
    "        val_acc = accuracy_from_logits(val_logits, yva_t)\n",
    "\n",
    "    if epoch % 200 == 0 or epoch == 1:\n",
    "        print(\n",
    "            f\"Epoch {epoch:4d} | \"\n",
    "            f\"train loss {total_loss / n_batches:.4f} | \"\n",
    "            f\"train acc {total_acc / n_batches:.3f} | \"\n",
    "            f\"val acc {val_acc:.3f}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbace0e2",
   "metadata": {},
   "source": [
    "Final report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abb753b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training accuracy:   0.967\n",
      "Final validation accuracy: 0.917\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    final_train_acc = accuracy_from_logits(model.forward(Xtr_t), ytr_t)\n",
    "    final_val_acc = accuracy_from_logits(model.forward(Xva_t), yva_t)\n",
    "\n",
    "print(f\"Final training accuracy:   {final_train_acc:.3f}\")\n",
    "print(f\"Final validation accuracy: {final_val_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1f29f2",
   "metadata": {},
   "source": [
    "#### Q6 (25 points): InstanceNorm vs BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d31b2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82805c5c",
   "metadata": {},
   "source": [
    "#### Q7 (25 points): InstanceNorm With and Without Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8334abad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (DS3000env)",
   "language": "python",
   "name": "ds3000env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
