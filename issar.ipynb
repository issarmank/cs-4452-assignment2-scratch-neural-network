{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "441dccaa",
   "metadata": {},
   "source": [
    "## Problem 1: Classification Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6dbeb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3c4112",
   "metadata": {},
   "source": [
    "#### Q1 (10 points): Implement Instance Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbdc219b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstanceNorm1D:\n",
    "    def __init__(self, D, eps=1e-5, device=None, dtype=torch.float32):\n",
    "        self.D = D\n",
    "        self.eps = eps\n",
    "\n",
    "        # parameters (learned)\n",
    "        self.gamma = torch.ones(D, device=device, dtype=dtype)\n",
    "        self.beta  = torch.zeros(D, device=device, dtype=dtype)\n",
    "\n",
    "        # gradients\n",
    "        self.dgamma = torch.zeros_like(self.gamma)\n",
    "        self.dbeta  = torch.zeros_like(self.beta)\n",
    "\n",
    "        # cache\n",
    "        self._cache = None\n",
    "\n",
    "    def forward(self, z):\n",
    "        assert z.dim() == 2 and z.size(1) == self.D, \"Expected (B, D) input\"\n",
    "\n",
    "        mu = z.mean(dim=1, keepdim=True)                         # (B, 1)\n",
    "        var = ((z - mu) ** 2).mean(dim=1, keepdim=True)          # (B, 1)\n",
    "        invstd = torch.rsqrt(var + self.eps)                     # (B, 1)\n",
    "        zhat = (z - mu) * invstd                                 # (B, D)\n",
    "\n",
    "        a = zhat * self.gamma.view(1, -1) + self.beta.view(1, -1)\n",
    "\n",
    "        # cache for backward\n",
    "        self._cache = (zhat, invstd)\n",
    "        return a\n",
    "\n",
    "    def backward(self, da):\n",
    "        zhat, invstd = self._cache\n",
    "        B, D = da.shape\n",
    "        assert D == self.D, \"Mismatched feature dimension\"\n",
    "\n",
    "        # parameter gradients\n",
    "        self.dbeta = da.sum(dim=0)               # (D,)\n",
    "        self.dgamma = (da * zhat).sum(dim=0)     # (D,)\n",
    "\n",
    "        # dL/dzhat\n",
    "        dzhat = da * self.gamma.view(1, -1)      # (B, D)\n",
    "\n",
    "        sum_dzhat = dzhat.sum(dim=1, keepdim=True)                # (B, 1)\n",
    "        sum_dzhat_zhat = (dzhat * zhat).sum(dim=1, keepdim=True)  # (B, 1)\n",
    "\n",
    "        dz = (invstd / D) * (D * dzhat - sum_dzhat - zhat * sum_dzhat_zhat)  # (B, D)\n",
    "        return dz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557a023d",
   "metadata": {},
   "source": [
    "#### Q2 (10 points): Implement Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fbf2aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, p=0.5):\n",
    "        assert 0.0 <= p < 1.0, \"p must be in [0, 1)\"\n",
    "        self.p = float(p)\n",
    "        self._mask = None\n",
    "        self._train = True  # default mode like PyTorch modules\n",
    "\n",
    "    def train(self):\n",
    "        self._train = True\n",
    "\n",
    "    def eval(self):\n",
    "        self._train = False\n",
    "\n",
    "    def forward(self, z):\n",
    "        if (not self._train) or self.p == 0.0:\n",
    "            self._mask = None\n",
    "            return z\n",
    "\n",
    "        keep_prob = 1.0 - self.p\n",
    "        # mask is 1 with prob keep_prob, else 0\n",
    "        self._mask = (torch.rand_like(z) < keep_prob).to(z.dtype)\n",
    "        a = (self._mask * z) / keep_prob\n",
    "        return a\n",
    "\n",
    "    def backward(self, da):\n",
    "        if (not self._train) or self.p == 0.0:\n",
    "            return da\n",
    "\n",
    "        keep_prob = 1.0 - self.p\n",
    "        dz = (self._mask / keep_prob) * da\n",
    "        return dz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0973410",
   "metadata": {},
   "source": [
    "#### Q3 (10 points): Implement Softmax Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32cf328",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxCrossEntropy:\n",
    "    def __init__(self):\n",
    "        self._cache = None  # (yhat, y)\n",
    "\n",
    "    def forward(self, z, y):\n",
    "        assert z.dim() == 2, \"z must be (B, C)\"\n",
    "        assert y.dim() == 1 and y.size(0) == z.size(0), \"y must be (B,)\"\n",
    "\n",
    "        B, C = z.shape\n",
    "\n",
    "        # stable softmax: subtract row-wise max\n",
    "        z_shift = z - z.max(dim=1, keepdim=True).values           # (B, C)\n",
    "        exp_z = torch.exp(z_shift)                                 # (B, C)\n",
    "        yhat = exp_z / exp_z.sum(dim=1, keepdim=True)              # (B, C)\n",
    "\n",
    "        # cross-entropy: -mean(log prob of correct class)\n",
    "        correct_probs = yhat[torch.arange(B, device=z.device), y]  # (B,)\n",
    "        loss = -torch.log(correct_probs + 1e-12).mean()            # scalar\n",
    "\n",
    "        self._cache = (yhat, y)\n",
    "        return loss\n",
    "\n",
    "    def backward(self):\n",
    "        yhat, y = self._cache\n",
    "        B, C = yhat.shape\n",
    "\n",
    "        dz = yhat.clone()\n",
    "        dz[torch.arange(B, device=dz.device), y] -= 1.0\n",
    "        dz /= B\n",
    "        return dz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a137fea6",
   "metadata": {},
   "source": [
    "#### Q4 (10 points): Implement Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2abf95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a95f1f9",
   "metadata": {},
   "source": [
    "#### Q5 (10 points): Build and Train a Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba71a16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f1f29f2",
   "metadata": {},
   "source": [
    "#### Q6 (25 points): InstanceNorm vs BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d31b2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82805c5c",
   "metadata": {},
   "source": [
    "#### Q7 (25 points): InstanceNorm With and Without Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8334abad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (DS3000env)",
   "language": "python",
   "name": "ds3000env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
